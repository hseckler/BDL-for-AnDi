{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d6440c",
   "metadata": {},
   "source": [
    "## Evaluations using the \"super\" dataset, to plot some dependencies on exponents, models or noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d8bf8",
   "metadata": {},
   "source": [
    "#### Here for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d2e6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from swag.posteriors import swag as swag\n",
    "from tqdm import tqdm\n",
    "\n",
    "from load_andi_dataset import *\n",
    "from LSTM_Neural_Network import *\n",
    "from swag_lr_scheduler import *\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ce5098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configer to only use gpu 1 not 0!\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6244f52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration, run on gpu if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dim = 1#2\n",
    "# Hyper-parameters, neuralnet parameters need to match the loaded model! \n",
    "input_dim = dim # 1D input sequence\n",
    "LSTM_size = [128,128,64]\n",
    "hidden_size = 20\n",
    "output_dim = 5 #output size\n",
    "batch_size = 500\n",
    "\n",
    "number_swags = 5 #number of swag models in multi swag ensemble\n",
    "number_mc_samples = 10 #number of samples taken per swag model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a5629fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a dataset for task(s) [2] and dimension(s) [1].\n",
      "Generating dataset for dimension 1.\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "#setup data using saved trajectories\n",
    "T = 100#500#100\n",
    "noise_T = T\n",
    "N_test = 100000\n",
    "N_save = 2000\n",
    "task = 2\n",
    "use_increments = True\n",
    "\n",
    "test_path = f\"datasets/trajectories/{dim}d/validset/\"\n",
    "if dim == 1:\n",
    "    test_path = f\"datasets/trajectories/validset/\"\n",
    "super_dataset = AnDi_super_dataset_from_saved_trajs(path = test_path, task = task, dim = dim, N_total = N_test, \n",
    "                                              T = T, N_save = N_save, use_increments = use_increments)\n",
    "print(len(super_dataset))\n",
    "#loader\n",
    "super_loader = torch.utils.data.DataLoader(dataset=super_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82ffa3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) tensor([0.2000]) tensor(0.1000) tensor([[ 3.1514],\n",
      "        [ 0.2321],\n",
      "        [ 0.0272],\n",
      "        [-0.1448],\n",
      "        [-0.0521]])\n"
     ]
    }
   ],
   "source": [
    "#check dataset example\n",
    "ex_models,ex_exponents,ex_noise,ex_traj = iter(super_loader).next()\n",
    "print(ex_models[0],ex_exponents[0],ex_noise[0],ex_traj[0][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2058a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load multiswag classification model:\n",
    "checkpoints_path = f\"saves/classi/{dim}d/{T}_lenght/multi/\"\n",
    "if dim == 1:\n",
    "    checkpoints_path = f\"saves/classi/{T}_lenght/multi/\"\n",
    "multi_swag_models = []\n",
    "for i in range(number_swags):\n",
    "    swag_model = swag.SWAG(LSTM_Classification, subspace_type = 'covariance', \n",
    "                       subspace_kwargs={'max_rank': 20}, num_input_features = input_dim, \n",
    "                       num_classes = output_dim, hidden_size = hidden_size, LSTM_size=LSTM_size)\n",
    "    swag_model.to(device)\n",
    "    swag_model.subspace.rank = torch.tensor(0)\n",
    "    \n",
    "    \n",
    "    name = \"swag_modelcheckpoint_multiswag%s\" % i\n",
    "    savefile = checkpoints_path + name\n",
    "    \n",
    "    swag_model.load_state_dict(torch.load(savefile,map_location=device))\n",
    "    swag_model.eval()\n",
    "    \n",
    "    \n",
    "    multi_swag_models.append(swag_model)\n",
    "    \n",
    "#crossentropy loss used for classification tasks and Softmax activation function\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "Softmax = torch.nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b611df34",
   "metadata": {},
   "source": [
    "#test multi swag model\n",
    "classes = ['attm', 'ctrw', 'fbm', 'lw', 'sbm']\n",
    "#plotting accuracy over confidence\n",
    "confidence = torch.arange(0,1,0.05).to(device) #confidence intervals\n",
    "accuracy_interval = torch.zeros(len(confidence)).to(device) #accuracy in each confidence interval\n",
    "n_interval = torch.zeros(len(confidence)).to(device) #number of samples for each confidence interval\n",
    "\n",
    "alpha_interval = torch.arange(0.05,2,0.1).to(device)\n",
    "acc_per_alpha = torch.zeros(len(alpha_interval)).to(device)\n",
    "pred_acc_per_alpha = torch.zeros(len(alpha_interval)).to(device)\n",
    "n_per_alpha = torch.zeros(len(alpha_interval)).to(device)\n",
    "\n",
    "acc_per_noise = torch.zeros(3).to(device) #acc per noise beeing 0.1,0.5,1\n",
    "n_per_noise = torch.zeros(3).to(device)\n",
    "confidence_per_noise = torch.zeros(3).to(device)\n",
    "\n",
    "all_gt_models = np.array([])\n",
    "all_confidences = np.array([])\n",
    "all_exponents = np.array([])\n",
    "all_noises = np.array([])\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    n_test_steps = len(super_loader)\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    acc_loss = 0\n",
    "    #acc_pred_var = 0\n",
    "    n_class_truepositive = np.zeros(output_dim)\n",
    "    n_class_falsepositive = np.zeros(output_dim)\n",
    "    n_class_falsenegative = np.zeros(output_dim)\n",
    "    conf_matrix = np.zeros((output_dim,output_dim))\n",
    "    \n",
    "    for labels, exponents, noise, traj in tqdm(super_loader):\n",
    "        all_models = np.append(all_gt_models,labels)\n",
    "        all_exponents = np.append(all_exponents,exponents)\n",
    "        all_noises = np.append(all_noises,noise)\n",
    "        \n",
    "        traj = traj.to(device)\n",
    "        labels = labels.to(device)\n",
    "        exponents = exponents.to(device)\n",
    "        noise = noise.to(device)\n",
    "        \n",
    "        output_samples = torch.ones(number_mc_samples*number_swags, len(traj), output_dim, dtype=torch.float32).to(device) \n",
    "        output_samples_prob = torch.ones(number_mc_samples*number_swags, len(traj), output_dim, dtype=torch.float32).to(device)\n",
    "        \n",
    "        for k in range(number_swags):\n",
    "            for i in range(number_mc_samples):\n",
    "                multi_swag_models[k].sample()\n",
    "                output_samples[i+k*number_mc_samples] = multi_swag_models[k](traj)\n",
    "                output_samples_prob[i+k*number_mc_samples] = Softmax(output_samples[i+k*number_mc_samples])\n",
    "        \n",
    "        outputs = output_samples.mean(0)\n",
    "        outputs_prob = output_samples_prob.mean(0)\n",
    "        #outputs_var = output_samples.var(0)\n",
    "        \n",
    "        #acc_pred_var += outputs_var.sum().item()\n",
    "        all_confidences = np.append(all_confidences,output_samples_prob.to(\"cpu\").detach().numpy())\n",
    "            \n",
    "        acc_loss += criterion(outputs, labels.view(-1)).item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs_prob.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted.view(-1) == labels.view(-1)).sum().item()\n",
    "        \n",
    "        \n",
    "        for i in range(len(traj)): #determine number of true/false negative/positives\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_truepositive[label] += 1\n",
    "            else:\n",
    "                n_class_falsepositive[pred] += 1\n",
    "                n_class_falsenegative[label] += 1\n",
    "            conf_matrix[pred,label] += 1\n",
    "            \n",
    "            #accuracy in the confidence interval\n",
    "            index = torch.where(confidence <= outputs_prob[i][pred])[-1][-1]\n",
    "            #print(index)\n",
    "            #print(outputs_prob[i][pred])\n",
    "            n_interval[index] += 1\n",
    "            if (label == pred):\n",
    "                accuracy_interval[index] += 1\n",
    "                \n",
    "            #accuracy in alpha interval\n",
    "            index_alpha = torch.where(alpha_interval <= exponents[i])[-1][-1]\n",
    "            n_per_alpha[index_alpha] += 1\n",
    "            if label == pred:\n",
    "                acc_per_alpha[index_alpha] += 1\n",
    "            pred_acc_per_alpha[index_alpha] += outputs_prob[i][pred]\n",
    "            \n",
    "            #accuracy per noise\n",
    "            index_noise = int(2*noise[i]+0.1)\n",
    "            n_per_noise[index_noise] += 1\n",
    "            if label == pred:\n",
    "                acc_per_noise[index_noise] += 1\n",
    "            confidence_per_noise[index_noise] += outputs_prob[i][pred].item()\n",
    "        \n",
    "    accuracy = n_correct/n_samples\n",
    "    mean_loss = acc_loss/n_test_steps\n",
    "    #mean_pred_var = acc_pred_var/n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test trajectories: {accuracy*100}%')\n",
    "    print(f'Mean loss is: {mean_loss}')\n",
    "    #print(f'Mean Variance predicted by SWAG is: {mean_pred_var}')\n",
    "    \n",
    "    class_precision = n_class_truepositive/(n_class_truepositive+n_class_falsepositive)\n",
    "    class_recall = n_class_truepositive/(n_class_truepositive+n_class_falsenegative)\n",
    "    class_f1_score = n_class_truepositive/(n_class_truepositive+0.5*(n_class_falsepositive+n_class_falsenegative))\n",
    "    \n",
    "    for i in range(output_dim):\n",
    "        print(f\"F1 score of class {classes[i]} is {class_f1_score[i]}\")\n",
    "    print(f\"Mean F1 score is {class_f1_score.mean()}\")\n",
    "\n",
    "\n",
    "    \n",
    "confidence = confidence.to(\"cpu\")\n",
    "accuracy_interval = (accuracy_interval/n_interval).to(\"cpu\")\n",
    "n_interval = n_interval.to(\"cpu\")\n",
    "alpha_interval = alpha_interval.to(\"cpu\")\n",
    "acc_per_alpha = (acc_per_alpha/n_per_alpha).to(\"cpu\")\n",
    "pred_acc_per_alpha = (pred_acc_per_alpha/n_per_alpha).to(\"cpu\")\n",
    "n_per_alpha = n_per_alpha.to(\"cpu\")\n",
    "acc_per_noise = (acc_per_noise/n_per_noise).to(\"cpu\")\n",
    "confidence_per_noise = (confidence_per_noise/n_per_noise).to(\"cpu\")\n",
    "n_per_noise = n_per_noise.to(\"cpu\")\n",
    "\n",
    "    \n",
    "#plot confusion matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "conf_matrix = conf_matrix/conf_matrix.sum(axis=0)\n",
    "\n",
    "\n",
    "df_cm = pd.DataFrame(conf_matrix, index = [i for i in classes],\n",
    "                  columns = [i for i in classes])\n",
    "# plt.figure(figsize=(7,7))\n",
    "#sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7106e45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 200/200 [00:46<00:00,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test trajectories: 44.879000000000005%\n",
      "Mean loss is: 1.2190670961141585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#test multi swag model\n",
    "classes = ['attm', 'ctrw', 'fbm', 'lw', 'sbm']\n",
    "#plotting accuracy over confidence\n",
    "confidence = torch.arange(0,1,0.05).to(device) #confidence intervals\n",
    "accuracy_interval = torch.zeros(len(confidence)).to(device) #accuracy in each confidence interval\n",
    "n_interval = torch.zeros(len(confidence)).to(device) #number of samples for each confidence interval\n",
    "\n",
    "alpha_interval = torch.arange(0.05,2,0.1).to(device)\n",
    "acc_per_alpha = torch.zeros(len(alpha_interval)).to(device)\n",
    "pred_acc_per_alpha = torch.zeros(len(alpha_interval)).to(device)\n",
    "n_per_alpha = torch.zeros(len(alpha_interval)).to(device)\n",
    "\n",
    "acc_per_noise = torch.zeros(3).to(device) #acc per noise beeing 0.1,0.5,1\n",
    "n_per_noise = torch.zeros(3).to(device)\n",
    "confidence_per_noise = torch.zeros(3).to(device)\n",
    "\n",
    "all_gt_models = np.array([])\n",
    "all_confidences = np.array([])\n",
    "all_exponents = np.array([])\n",
    "all_noises = np.array([])\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    n_test_steps = len(super_loader)\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    acc_loss = 0\n",
    "    #acc_pred_var = 0\n",
    "    n_class_truepositive = np.zeros(output_dim)\n",
    "    n_class_falsepositive = np.zeros(output_dim)\n",
    "    n_class_falsenegative = np.zeros(output_dim)\n",
    "    conf_matrix = np.zeros((output_dim,output_dim))\n",
    "    \n",
    "    for labels, exponents, noise, traj in tqdm(super_loader):\n",
    "        all_gt_models = np.append(all_gt_models,labels)\n",
    "        all_exponents = np.append(all_exponents,exponents)\n",
    "        all_noises = np.append(all_noises,noise)\n",
    "        \n",
    "        traj = traj.to(device)\n",
    "        labels = labels.to(device)\n",
    "        exponents = exponents.to(device)\n",
    "        noise = noise.to(device)\n",
    "        \n",
    "        output_samples = torch.ones(number_mc_samples*number_swags, len(traj), output_dim, dtype=torch.float32).to(device) \n",
    "        output_samples_prob = torch.ones(number_mc_samples*number_swags, len(traj), output_dim, dtype=torch.float32).to(device)\n",
    "        \n",
    "        for k in range(number_swags):\n",
    "            for i in range(number_mc_samples):\n",
    "                multi_swag_models[k].sample()\n",
    "                output_samples[i+k*number_mc_samples] = multi_swag_models[k](traj)\n",
    "                output_samples_prob[i+k*number_mc_samples] = Softmax(output_samples[i+k*number_mc_samples])\n",
    "        \n",
    "        outputs = output_samples.mean(0)\n",
    "        outputs_prob = output_samples_prob.mean(0)\n",
    "        #outputs_var = output_samples.var(0)\n",
    "        \n",
    "        #acc_pred_var += outputs_var.sum().item()\n",
    "        all_confidences = np.append(all_confidences,outputs_prob.to(\"cpu\").detach().numpy())\n",
    "            \n",
    "        acc_loss += criterion(outputs, labels.view(-1)).item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs_prob.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted.view(-1) == labels.view(-1)).sum().item()\n",
    "        \n",
    "    accuracy = n_correct/n_samples\n",
    "    mean_loss = acc_loss/n_test_steps\n",
    "    #mean_pred_var = acc_pred_var/n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test trajectories: {accuracy*100}%')\n",
    "    print(f'Mean loss is: {mean_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6566b873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.74495453e-01 7.25104630e-01 1.04829349e-04 ... 2.55500495e-01\n",
      " 8.40612203e-02 2.61290461e-01]\n",
      "0.9999999794326868\n",
      "0.9999999925494194\n",
      "[2.74495453e-01 7.25104630e-01 1.04829349e-04 2.95806476e-05\n",
      " 2.65486800e-04]\n",
      "[[1.00000000e+00 1.00000000e+00 2.00000000e+00 ... 2.00000000e+00\n",
      "  0.00000000e+00 2.00000000e+00]\n",
      " [2.74495453e-01 4.36530143e-01 2.93075264e-01 ... 2.05752581e-01\n",
      "  1.49572194e-01 2.26571560e-01]\n",
      " [7.25104630e-01 5.47837675e-01 1.36475965e-01 ... 3.09181772e-02\n",
      "  3.34526122e-01 1.72576278e-01]\n",
      " ...\n",
      " [2.65486800e-04 1.03825042e-02 2.93116152e-01 ... 3.14254194e-01\n",
      "  2.11854771e-01 2.61290461e-01]\n",
      " [2.00000003e-01 4.49999988e-01 1.60000002e+00 ... 1.95000005e+00\n",
      "  8.00000012e-01 4.00000006e-01]\n",
      " [1.00000001e-01 1.00000001e-01 1.00000001e-01 ... 1.00000000e+00\n",
      "  5.00000000e-01 5.00000000e-01]]\n",
      "done 10\n"
     ]
    }
   ],
   "source": [
    "all_gt_models\n",
    "print(all_confidences)\n",
    "print(all_confidences[0]+all_confidences[1]+all_confidences[2]+all_confidences[3]+all_confidences[4])\n",
    "all_exponents\n",
    "all_noises\n",
    "all_confidences = all_confidences.reshape(100000,5)\n",
    "print(sum(all_confidences[2]))\n",
    "print(all_confidences[0])\n",
    "plotdata = np.asarray([all_gt_models,all_confidences[:,0],all_confidences[:,1],all_confidences[:,2],all_confidences[:,3],all_confidences[:,4],all_exponents,all_noises])\n",
    "print(plotdata)\n",
    "print(f\"done {T}\")\n",
    "savename = \"plotdata/\"+f\"{dim}d_classification_length{T}\"\n",
    "if dim  == 1:\n",
    "    savename = \"plotdata/\"+f\"classification_length{T}\"\n",
    "np.savetxt(savename,plotdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e590552a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "061bbffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 11})\n",
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "plt.rc('axes', titlesize=14)\n",
    "plt.rc('axes', labelsize=16)  \n",
    "\n",
    "plt.plot(alpha_interval+0.05,acc_per_alpha*100,\"o-\",linewidth=0.7,label=\"observed\")\n",
    "plt.plot(alpha_interval+0.05,pred_acc_per_alpha*100,\"o-\",linewidth=0.7,label=\"predicted\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Ground Truth Anomalous Exponent\")\n",
    "plt.ylabel(\"Accuracy %\")\n",
    "plt.legend()\n",
    "\n",
    "#use fitting save directory!\n",
    "#plt.savefig(\"figs/super_evaluate/class_acc_over_exp_all_models_predicted.png\")\n",
    "#plt.savefig(\"figs/super_evaluate/class_acc_over_exp_all_models_predicted.svg\")\n",
    "\n",
    "#plt.savefig(\"figs/super_evaluate/correctnoise/class_acc_over_exp_all_models_predicted.png\")\n",
    "#plt.savefig(\"figs/super_evaluate/correctnoise/class_acc_over_exp_all_models_predicted.svg\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da19121",
   "metadata": {},
   "outputs": [],
   "source": [
    "snr = [10,2,1]\n",
    "x_labels = [\"SNR 10\", \"SNR 2\", \"SNR 1\"]\n",
    "x_val = [0,1,2]\n",
    "\n",
    "plt.plot(x_val,acc_per_noise*100,\"o\",ms = 11,label=\"Observed\")\n",
    "plt.plot(x_val,confidence_per_noise*100,\"o\",ms = 11,label=\"Predicted\")\n",
    "plt.xticks(ticks = x_val, labels = x_labels, size = 16)\n",
    "print(acc_per_noise)\n",
    "print(confidence_per_noise)\n",
    "plt.ylabel(\"Accuracy %\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "#use fitting save directory!\n",
    "#plt.savefig(\"figs/super_evaluate/class_acc_over_noise_all_models.png\")\n",
    "#plt.savefig(\"figs/super_evaluate/class_acc_over_noise_all_models.svg\")\n",
    "\n",
    "#plt.savefig(\"figs/super_evaluate/correctnoise/class_acc_over_noise_all_models.png\")\n",
    "#plt.savefig(\"figs/super_evaluate/correctnoise/class_acc_over_noise_all_models.svg\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(confidence+0.025,accuracy_interval,\"o-\")\n",
    "#plt.plot(confidence,accuracy_interval_multiswa,\"ro-\")\n",
    "plt.plot(np.arange(0,1.05,0.05),np.arange(0,1.05,0.05),\"grey\")\n",
    "plt.xlabel(\"Confidence\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "#plt.savefig(\"figs/super_evaluate/conf_acc_all_models.png\")\n",
    "#plt.savefig(\"figs/super_evaluate/conf_acc_all_models.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a35e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
