{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d6440c",
   "metadata": {},
   "source": [
    "## Evaluations using the \"super\" dataset, to plot some dependencies on exponents, models or noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d8bf8",
   "metadata": {},
   "source": [
    "#### Here for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d2e6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from swag.posteriors import swag as swag\n",
    "from tqdm import tqdm\n",
    "\n",
    "from load_andi_dataset import *\n",
    "from LSTM_Neural_Network import *\n",
    "from swag_lr_scheduler import *\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ce5098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configer to only use gpu 1 not 0!\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6244f52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration, run on gpu if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dim = 1#2\n",
    "# Hyper-parameters, neuralnet parameters need to match the loaded model! \n",
    "input_dim = dim # 1D input sequence\n",
    "LSTM_size = [128,128,64]\n",
    "hidden_size = 20\n",
    "output_dim = 5 #output size\n",
    "batch_size = 500\n",
    "\n",
    "number_swags = 5 #number of swag models in multi swag ensemble\n",
    "number_mc_samples = 10 #number of samples taken per swag model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a5629fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a dataset for task(s) [2] and dimension(s) [1].\n",
      "Generating dataset for dimension 1.\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "#setup data using saved trajectories\n",
    "T = 100#500#100\n",
    "noise_T = T\n",
    "N_test = 100000\n",
    "N_save = 2000\n",
    "task = 2\n",
    "use_increments = True\n",
    "\n",
    "test_path = f\"datasets/trajectories/{dim}d/validset/\"\n",
    "if dim == 1:\n",
    "    test_path = f\"datasets/trajectories/validset/\"\n",
    "super_dataset = AnDi_super_dataset_from_saved_trajs(path = test_path, task = task, dim = dim, N_total = N_test, \n",
    "                                              T = T, N_save = N_save, use_increments = use_increments)\n",
    "print(len(super_dataset))\n",
    "#loader\n",
    "super_loader = torch.utils.data.DataLoader(dataset=super_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82ffa3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) tensor([0.2000]) tensor(0.1000) tensor([[ 3.1514],\n",
      "        [ 0.2321],\n",
      "        [ 0.0272],\n",
      "        [-0.1448],\n",
      "        [-0.0521]])\n"
     ]
    }
   ],
   "source": [
    "#check dataset example\n",
    "ex_models,ex_exponents,ex_noise,ex_traj = iter(super_loader).next()\n",
    "print(ex_models[0],ex_exponents[0],ex_noise[0],ex_traj[0][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2058a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load multiswag classification model:\n",
    "checkpoints_path = f\"saves/classi/{dim}d/{T}_lenght/multi/\"\n",
    "if dim == 1:\n",
    "    checkpoints_path = f\"saves/classi/{T}_lenght/multi/\"\n",
    "multi_swag_models = []\n",
    "for i in range(number_swags):\n",
    "    swag_model = swag.SWAG(LSTM_Classification, subspace_type = 'covariance', \n",
    "                       subspace_kwargs={'max_rank': 20}, num_input_features = input_dim, \n",
    "                       num_classes = output_dim, hidden_size = hidden_size, LSTM_size=LSTM_size)\n",
    "    swag_model.to(device)\n",
    "    swag_model.subspace.rank = torch.tensor(0)\n",
    "    \n",
    "    \n",
    "    name = \"swag_modelcheckpoint_multiswag%s\" % i\n",
    "    savefile = checkpoints_path + name\n",
    "    \n",
    "    swag_model.load_state_dict(torch.load(savefile,map_location=device))\n",
    "    swag_model.eval()\n",
    "    \n",
    "    \n",
    "    multi_swag_models.append(swag_model)\n",
    "    \n",
    "#crossentropy loss used for classification tasks and Softmax activation function\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "Softmax = torch.nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b611df34",
   "metadata": {},
   "source": [
    "#test multi swag model\n",
    "classes = ['attm', 'ctrw', 'fbm', 'lw', 'sbm']\n",
    "#plotting accuracy over confidence\n",
    "confidence = torch.arange(0,1,0.05).to(device) #confidence intervals\n",
    "accuracy_interval = torch.zeros(len(confidence)).to(device) #accuracy in each confidence interval\n",
    "n_interval = torch.zeros(len(confidence)).to(device) #number of samples for each confidence interval\n",
    "\n",
    "alpha_interval = torch.arange(0.05,2,0.1).to(device)\n",
    "acc_per_alpha = torch.zeros(len(alpha_interval)).to(device)\n",
    "pred_acc_per_alpha = torch.zeros(len(alpha_interval)).to(device)\n",
    "n_per_alpha = torch.zeros(len(alpha_interval)).to(device)\n",
    "\n",
    "acc_per_noise = torch.zeros(3).to(device) #acc per noise beeing 0.1,0.5,1\n",
    "n_per_noise = torch.zeros(3).to(device)\n",
    "confidence_per_noise = torch.zeros(3).to(device)\n",
    "\n",
    "all_gt_models = np.array([])\n",
    "all_confidences = np.array([])\n",
    "all_exponents = np.array([])\n",
    "all_noises = np.array([])\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    n_test_steps = len(super_loader)\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    acc_loss = 0\n",
    "    #acc_pred_var = 0\n",
    "    n_class_truepositive = np.zeros(output_dim)\n",
    "    n_class_falsepositive = np.zeros(output_dim)\n",
    "    n_class_falsenegative = np.zeros(output_dim)\n",
    "    conf_matrix = np.zeros((output_dim,output_dim))\n",
    "    \n",
    "    for labels, exponents, noise, traj in tqdm(super_loader):\n",
    "        all_models = np.append(all_gt_models,labels)\n",
    "        all_exponents = np.append(all_exponents,exponents)\n",
    "        all_noises = np.append(all_noises,noise)\n",
    "        \n",
    "        traj = traj.to(device)\n",
    "        labels = labels.to(device)\n",
    "        exponents = exponents.to(device)\n",
    "        noise = noise.to(device)\n",
    "        \n",
    "        output_samples = torch.ones(number_mc_samples*number_swags, len(traj), output_dim, dtype=torch.float32).to(device) \n",
    "        output_samples_prob = torch.ones(number_mc_samples*number_swags, len(traj), output_dim, dtype=torch.float32).to(device)\n",
    "        \n",
    "        for k in range(number_swags):\n",
    "            for i in range(number_mc_samples):\n",
    "                multi_swag_models[k].sample()\n",
    "                output_samples[i+k*number_mc_samples] = multi_swag_models[k](traj)\n",
    "                output_samples_prob[i+k*number_mc_samples] = Softmax(output_samples[i+k*number_mc_samples])\n",
    "        \n",
    "        outputs = output_samples.mean(0)\n",
    "        outputs_prob = output_samples_prob.mean(0)\n",
    "        #outputs_var = output_samples.var(0)\n",
    "        \n",
    "        #acc_pred_var += outputs_var.sum().item()\n",
    "        all_confidences = np.append(all_confidences,output_samples_prob.to(\"cpu\").detach().numpy())\n",
    "            \n",
    "        acc_loss += criterion(outputs, labels.view(-1)).item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs_prob.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted.view(-1) == labels.view(-1)).sum().item()\n",
    "        \n",
    "        \n",
    "        for i in range(len(traj)): #determine number of true/false negative/positives\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_truepositive[label] += 1\n",
    "            else:\n",
    "                n_class_falsepositive[pred] += 1\n",
    "                n_class_falsenegative[label] += 1\n",
    "            conf_matrix[pred,label] += 1\n",
    "            \n",
    "            #accuracy in the confidence interval\n",
    "            index = torch.where(confidence <= outputs_prob[i][pred])[-1][-1]\n",
    "            #print(index)\n",
    "            #print(outputs_prob[i][pred])\n",
    "            n_interval[index] += 1\n",
    "            if (label == pred):\n",
    "                accuracy_interval[index] += 1\n",
    "                \n",
    "            #accuracy in alpha interval\n",
    "            index_alpha = torch.where(alpha_interval <= exponents[i])[-1][-1]\n",
    "            n_per_alpha[index_alpha] += 1\n",
    "            if label == pred:\n",
    "                acc_per_alpha[index_alpha] += 1\n",
    "            pred_acc_per_alpha[index_alpha] += outputs_prob[i][pred]\n",
    "            \n",
    "            #accuracy per noise\n",
    "            index_noise = int(2*noise[i]+0.1)\n",
    "            n_per_noise[index_noise] += 1\n",
    "            if label == pred:\n",
    "                acc_per_noise[index_noise] += 1\n",
    "            confidence_per_noise[index_noise] += outputs_prob[i][pred].item()\n",
    "        \n",
    "    accuracy = n_correct/n_samples\n",
    "    mean_loss = acc_loss/n_test_steps\n",
    "    #mean_pred_var = acc_pred_var/n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test trajectories: {accuracy*100}%')\n",
    "    print(f'Mean loss is: {mean_loss}')\n",
    "    #print(f'Mean Variance predicted by SWAG is: {mean_pred_var}')\n",
    "    \n",
    "    class_precision = n_class_truepositive/(n_class_truepositive+n_class_falsepositive)\n",
    "    class_recall = n_class_truepositive/(n_class_truepositive+n_class_falsenegative)\n",
    "    class_f1_score = n_class_truepositive/(n_class_truepositive+0.5*(n_class_falsepositive+n_class_falsenegative))\n",
    "    \n",
    "    for i in range(output_dim):\n",
    "        print(f\"F1 score of class {classes[i]} is {class_f1_score[i]}\")\n",
    "    print(f\"Mean F1 score is {class_f1_score.mean()}\")\n",
    "\n",
    "\n",
    "    \n",
    "confidence = confidence.to(\"cpu\")\n",
    "accuracy_interval = (accuracy_interval/n_interval).to(\"cpu\")\n",
    "n_interval = n_interval.to(\"cpu\")\n",
    "alpha_interval = alpha_interval.to(\"cpu\")\n",
    "acc_per_alpha = (acc_per_alpha/n_per_alpha).to(\"cpu\")\n",
    "pred_acc_per_alpha = (pred_acc_per_alpha/n_per_alpha).to(\"cpu\")\n",
    "n_per_alpha = n_per_alpha.to(\"cpu\")\n",
    "acc_per_noise = (acc_per_noise/n_per_noise).to(\"cpu\")\n",
    "confidence_per_noise = (confidence_per_noise/n_per_noise).to(\"cpu\")\n",
    "n_per_noise = n_per_noise.to(\"cpu\")\n",
    "\n",
    "    \n",
    "#plot confusion matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "conf_matrix = conf_matrix/conf_matrix.sum(axis=0)\n",
    "\n",
    "\n",
    "df_cm = pd.DataFrame(conf_matrix, index = [i for i in classes],\n",
    "                  columns = [i for i in classes])\n",
    "# plt.figure(figsize=(7,7))\n",
    "#sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7106e45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 200/200 [00:46<00:00,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test trajectories: 44.879000000000005%\n",
      "Mean loss is: 1.2190670961141585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#test multi swag model\n",
    "classes = ['attm', 'ctrw', 'fbm', 'lw', 'sbm']\n",
    "#plotting accuracy over confidence\n",
    "confidence = torch.arange(0,1,0.05).to(device) #confidence intervals\n",
    "accuracy_interval = torch.zeros(len(confidence)).to(device) #accuracy in each confidence interval\n",
    "n_interval = torch.zeros(len(confidence)).to(device) #number of samples for each confidence interval\n",
    "\n",
    "alpha_interval = torch.arange(0.05,2,0.1).to(device)\n",
    "acc_per_alpha = torch.zeros(len(alpha_interval)).to(device)\n",
    "pred_acc_per_alpha = torch.zeros(len(alpha_interval)).to(device)\n",
    "n_per_alpha = torch.zeros(len(alpha_interval)).to(device)\n",
    "\n",
    "acc_per_noise = torch.zeros(3).to(device) #acc per noise beeing 0.1,0.5,1\n",
    "n_per_noise = torch.zeros(3).to(device)\n",
    "confidence_per_noise = torch.zeros(3).to(device)\n",
    "\n",
    "all_gt_models = np.array([])\n",
    "all_confidences = np.array([])\n",
    "all_exponents = np.array([])\n",
    "all_noises = np.array([])\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    n_test_steps = len(super_loader)\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    acc_loss = 0\n",
    "    #acc_pred_var = 0\n",
    "    n_class_truepositive = np.zeros(output_dim)\n",
    "    n_class_falsepositive = np.zeros(output_dim)\n",
    "    n_class_falsenegative = np.zeros(output_dim)\n",
    "    conf_matrix = np.zeros((output_dim,output_dim))\n",
    "    \n",
    "    for labels, exponents, noise, traj in tqdm(super_loader):\n",
    "        all_gt_models = np.append(all_gt_models,labels)\n",
    "        all_exponents = np.append(all_exponents,exponents)\n",
    "        all_noises = np.append(all_noises,noise)\n",
    "        \n",
    "        traj = traj.to(device)\n",
    "        labels = labels.to(device)\n",
    "        exponents = exponents.to(device)\n",
    "        noise = noise.to(device)\n",
    "        \n",
    "        output_samples = torch.ones(number_mc_samples*number_swags, len(traj), output_dim, dtype=torch.float32).to(device) \n",
    "        output_samples_prob = torch.ones(number_mc_samples*number_swags, len(traj), output_dim, dtype=torch.float32).to(device)\n",
    "        \n",
    "        for k in range(number_swags):\n",
    "            for i in range(number_mc_samples):\n",
    "                multi_swag_models[k].sample()\n",
    "                output_samples[i+k*number_mc_samples] = multi_swag_models[k](traj)\n",
    "                output_samples_prob[i+k*number_mc_samples] = Softmax(output_samples[i+k*number_mc_samples])\n",
    "        \n",
    "        outputs = output_samples.mean(0)\n",
    "        outputs_prob = output_samples_prob.mean(0)\n",
    "        #outputs_var = output_samples.var(0)\n",
    "        \n",
    "        #acc_pred_var += outputs_var.sum().item()\n",
    "        all_confidences = np.append(all_confidences,outputs_prob.to(\"cpu\").detach().numpy())\n",
    "            \n",
    "        acc_loss += criterion(outputs, labels.view(-1)).item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs_prob.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted.view(-1) == labels.view(-1)).sum().item()\n",
    "        \n",
    "    accuracy = n_correct/n_samples\n",
    "    mean_loss = acc_loss/n_test_steps\n",
    "    #mean_pred_var = acc_pred_var/n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test trajectories: {accuracy*100}%')\n",
    "    print(f'Mean loss is: {mean_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6566b873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.74495453e-01 7.25104630e-01 1.04829349e-04 ... 2.55500495e-01\n",
      " 8.40612203e-02 2.61290461e-01]\n",
      "0.9999999794326868\n",
      "0.9999999925494194\n",
      "[2.74495453e-01 7.25104630e-01 1.04829349e-04 2.95806476e-05\n",
      " 2.65486800e-04]\n",
      "[[1.00000000e+00 1.00000000e+00 2.00000000e+00 ... 2.00000000e+00\n",
      "  0.00000000e+00 2.00000000e+00]\n",
      " [2.74495453e-01 4.36530143e-01 2.93075264e-01 ... 2.05752581e-01\n",
      "  1.49572194e-01 2.26571560e-01]\n",
      " [7.25104630e-01 5.47837675e-01 1.36475965e-01 ... 3.09181772e-02\n",
      "  3.34526122e-01 1.72576278e-01]\n",
      " ...\n",
      " [2.65486800e-04 1.03825042e-02 2.93116152e-01 ... 3.14254194e-01\n",
      "  2.11854771e-01 2.61290461e-01]\n",
      " [2.00000003e-01 4.49999988e-01 1.60000002e+00 ... 1.95000005e+00\n",
      "  8.00000012e-01 4.00000006e-01]\n",
      " [1.00000001e-01 1.00000001e-01 1.00000001e-01 ... 1.00000000e+00\n",
      "  5.00000000e-01 5.00000000e-01]]\n",
      "done 10\n"
     ]
    }
   ],
   "source": [
    "all_gt_models\n",
    "print(all_confidences)\n",
    "print(all_confidences[0]+all_confidences[1]+all_confidences[2]+all_confidences[3]+all_confidences[4])\n",
    "all_exponents\n",
    "all_noises\n",
    "all_confidences = all_confidences.reshape(100000,5)\n",
    "print(sum(all_confidences[2]))\n",
    "print(all_confidences[0])\n",
    "plotdata = np.asarray([all_gt_models,all_confidences[:,0],all_confidences[:,1],all_confidences[:,2],all_confidences[:,3],all_confidences[:,4],all_exponents,all_noises])\n",
    "print(plotdata)\n",
    "print(f\"done {T}\")\n",
    "savename = \"plotdata/\"+f\"{dim}d_classification_length{T}\"\n",
    "if dim  == 1:\n",
    "    savename = \"plotdata/\"+f\"classification_length{T}\"\n",
    "np.savetxt(savename,plotdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e590552a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "061bbffb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30773/994354526.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha_interval\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_per_alpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"o-\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"observed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha_interval\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_acc_per_alpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"o-\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"predicted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_gpu/lib/python3.9/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3017\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   3020\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_gpu/lib/python3.9/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1605\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1606\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1607\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1608\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_autoscale_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_gpu/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36madd_line\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   2099\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_line_limits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_line%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_gpu/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_update_line_limits\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         \u001b[0mFigures\u001b[0m \u001b[0mout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdating\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataLim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2122\u001b[0m         \"\"\"\n\u001b[0;32m-> 2123\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2125\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_gpu/lib/python3.9/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mget_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \"\"\"\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_gpu/lib/python3.9/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mrecache\u001b[0;34m(self, always)\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0malways\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0mxconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_xunits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xorig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_unmasked_float_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_gpu/lib/python3.9/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_to_unmasked_float_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1331\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1333\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_gpu/lib/python3.9/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order, like)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_gpu/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;31m# Wrap Numpy array again in a suitable tensor when done, to support e.g.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams.update({'font.size': 11})\n",
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "plt.rc('axes', titlesize=14)\n",
    "plt.rc('axes', labelsize=16)  \n",
    "\n",
    "plt.plot(alpha_interval+0.05,acc_per_alpha*100,\"o-\",linewidth=0.7,label=\"observed\")\n",
    "plt.plot(alpha_interval+0.05,pred_acc_per_alpha*100,\"o-\",linewidth=0.7,label=\"predicted\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Ground Truth Anomalous Exponent\")\n",
    "plt.ylabel(\"Accuracy %\")\n",
    "plt.legend()\n",
    "\n",
    "#use fitting save directory!\n",
    "#plt.savefig(\"figs/super_evaluate/class_acc_over_exp_all_models_predicted.png\")\n",
    "#plt.savefig(\"figs/super_evaluate/class_acc_over_exp_all_models_predicted.svg\")\n",
    "\n",
    "#plt.savefig(\"figs/super_evaluate/correctnoise/class_acc_over_exp_all_models_predicted.png\")\n",
    "#plt.savefig(\"figs/super_evaluate/correctnoise/class_acc_over_exp_all_models_predicted.svg\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da19121",
   "metadata": {},
   "outputs": [],
   "source": [
    "snr = [10,2,1]\n",
    "x_labels = [\"SNR 10\", \"SNR 2\", \"SNR 1\"]\n",
    "x_val = [0,1,2]\n",
    "\n",
    "plt.plot(x_val,acc_per_noise*100,\"o\",ms = 11,label=\"Observed\")\n",
    "plt.plot(x_val,confidence_per_noise*100,\"o\",ms = 11,label=\"Predicted\")\n",
    "plt.xticks(ticks = x_val, labels = x_labels, size = 16)\n",
    "print(acc_per_noise)\n",
    "print(confidence_per_noise)\n",
    "plt.ylabel(\"Accuracy %\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "#use fitting save directory!\n",
    "#plt.savefig(\"figs/super_evaluate/class_acc_over_noise_all_models.png\")\n",
    "#plt.savefig(\"figs/super_evaluate/class_acc_over_noise_all_models.svg\")\n",
    "\n",
    "#plt.savefig(\"figs/super_evaluate/correctnoise/class_acc_over_noise_all_models.png\")\n",
    "#plt.savefig(\"figs/super_evaluate/correctnoise/class_acc_over_noise_all_models.svg\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(confidence+0.025,accuracy_interval,\"o-\")\n",
    "#plt.plot(confidence,accuracy_interval_multiswa,\"ro-\")\n",
    "plt.plot(np.arange(0,1.05,0.05),np.arange(0,1.05,0.05),\"grey\")\n",
    "plt.xlabel(\"Confidence\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "#plt.savefig(\"figs/super_evaluate/conf_acc_all_models.png\")\n",
    "#plt.savefig(\"figs/super_evaluate/conf_acc_all_models.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dffc68",
   "metadata": {},
   "source": [
    "### Classifcation fbm and sbm only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456a703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from swag.posteriors import swag as swag\n",
    "import tqdm\n",
    "\n",
    "%run load_andi_dataset.ipynb\n",
    "%run LSTM_Neural_Network.ipynb\n",
    "%run swag_lr_scheduler.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cdc3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration, run on gpu if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters, neuralnet parameters need to match the loaded model! \n",
    "input_dim = 1 # 1D input sequence\n",
    "LSTM_size = 64\n",
    "hidden_size = 20\n",
    "output_dim = 5 #output size\n",
    "batch_size = 500\n",
    "\n",
    "number_swags = 5 #number of swag models in multi swag ensemble\n",
    "number_mc_samples = 10 #number of samples taken per swag model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d5e2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "super_dataset = AnDi_super_dataset(path = \"datasets/super/1dim_100lenght/\", \n",
    "                                   fname = \"andiset50000_sbmfbm.txt\", dim=1)\n",
    "#super_dataset = AnDi_super_dataset(path = \"datasets/super/1dim_100lenght/\", \n",
    "#                                   fname = \"andiset50000_sbmfbm_correctnoise.txt\", dim=1)\n",
    "\n",
    "super_loader = torch.utils.data.DataLoader(dataset=super_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7458f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load multiswag classification model:\n",
    "checkpoints_path = \"saves/classi/multi/highswalr/\"\n",
    "\n",
    "multi_swag_models = []\n",
    "for i in range(number_swags):\n",
    "    swag_model = swag.SWAG(LSTM_Classification, subspace_type = 'covariance', \n",
    "                       subspace_kwargs={'max_rank': 20}, num_input_features = input_dim, \n",
    "                       num_classes = output_dim, hidden_size = hidden_size, LSTM_size=LSTM_size)\n",
    "    swag_model.to(device)\n",
    "    swag_model.subspace.rank = torch.tensor(0)\n",
    "    \n",
    "    \n",
    "    name = \"swag_modelcheckpoint_multiswag%s\" % i\n",
    "    savefile = checkpoints_path + name\n",
    "    \n",
    "    swag_model.load_state_dict(torch.load(savefile,map_location=device))\n",
    "    swag_model.eval()\n",
    "    \n",
    "    \n",
    "    multi_swag_models.append(swag_model)\n",
    "    \n",
    "#crossentropy loss used for classification tasks and Softmax activation function\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "Softmax = torch.nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d574878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test multi swag model\n",
    "classes = ['attm', 'ctrw', 'fbm', 'lw', 'sbm']\n",
    "#plotting accuracy over confidence\n",
    "confidence = torch.arange(0,1,0.05).to(device) #confidence intervals\n",
    "accuracy_interval = torch.zeros(len(confidence)).to(device) #accuracy in each confidence interval\n",
    "n_interval = torch.zeros(len(confidence)).to(device) #number of samples for each confidence interval\n",
    "\n",
    "alpha_interval = torch.arange(0.05,2,0.1).to(device)\n",
    "acc_per_alpha = torch.zeros(len(alpha_interval)).to(device)\n",
    "pred_acc_per_alpha = torch.zeros(len(alpha_interval)).to(device)\n",
    "n_per_alpha = torch.zeros(len(alpha_interval)).to(device)\n",
    "\n",
    "acc_per_noise = torch.zeros(3).to(device) #acc per noise beeing 0.1,0.5,1\n",
    "n_per_noise = torch.zeros(3).to(device)\n",
    "confidence_per_noise = torch.zeros(3).to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    n_test_steps = len(super_loader)\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    acc_loss = 0\n",
    "    #acc_pred_var = 0\n",
    "    n_class_truepositive = np.zeros(output_dim)\n",
    "    n_class_falsepositive = np.zeros(output_dim)\n",
    "    n_class_falsenegative = np.zeros(output_dim)\n",
    "    conf_matrix = np.zeros((output_dim,output_dim))\n",
    "    \n",
    "    for labels, exponents, noise, traj in tqdm.tqdm(super_loader):\n",
    "        \n",
    "        traj = traj.to(device)\n",
    "        labels = labels.to(device)\n",
    "        exponents = exponents.to(device)\n",
    "        noise = noise.to(device)\n",
    "        \n",
    "        output_samples = torch.ones(number_mc_samples*number_swags, len(traj), output_dim, dtype=torch.float32).to(device) \n",
    "        output_samples_prob = torch.ones(number_mc_samples*number_swags, len(traj), output_dim, dtype=torch.float32).to(device)\n",
    "        \n",
    "        for k in range(number_swags):\n",
    "            for i in range(number_mc_samples):\n",
    "                multi_swag_models[k].sample()\n",
    "                output_samples[i+k*number_mc_samples] = multi_swag_models[k](traj)\n",
    "                output_samples_prob[i+k*number_mc_samples] = Softmax(output_samples[i+k*number_mc_samples])\n",
    "        \n",
    "        outputs = output_samples.mean(0)\n",
    "        outputs_prob = output_samples_prob.mean(0)\n",
    "        #outputs_var = output_samples.var(0)\n",
    "        \n",
    "        #acc_pred_var += outputs_var.sum().item()\n",
    "            \n",
    "            \n",
    "        acc_loss += criterion(outputs, labels.view(-1)).item()\n",
    "        \n",
    "        #allow for sbm, fbm predictions only!\n",
    "        sbm_pred = outputs_prob[:,4] > outputs_prob[:,2] \n",
    "        predicted = sbm_pred*2 + 2 #should be 2 for fbm and 4 for sbm?\n",
    "        #_, predicted = torch.max(outputs_prob.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted.view(-1) == labels.view(-1)).sum().item()\n",
    "        \n",
    "        \n",
    "        for i in range(len(traj)): #determine number of true/false negative/positives\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_truepositive[label] += 1\n",
    "            else:\n",
    "                n_class_falsepositive[pred] += 1\n",
    "                n_class_falsenegative[label] += 1\n",
    "            conf_matrix[pred,label] += 1\n",
    "            \n",
    "            #accuracy in the confidence interval, adjust confidence to differentiate between sbm and fbm only\n",
    "            sbmfbm_prob = outputs_prob[i][pred].item()/(outputs_prob[i][2].item() + outputs_prob[i][4].item())\n",
    "            index = torch.where(confidence <= sbmfbm_prob)[-1][-1]\n",
    "            #print(index)\n",
    "            #print(outputs_prob[i][pred])\n",
    "            n_interval[index] += 1\n",
    "            if (label == pred):\n",
    "                accuracy_interval[index] += 1\n",
    "                \n",
    "            #accuracy in alpha interval\n",
    "            index_alpha = torch.where(alpha_interval <= exponents[i])[-1][-1]\n",
    "            n_per_alpha[index_alpha] += 1\n",
    "            if label == pred:\n",
    "                acc_per_alpha[index_alpha] += 1\n",
    "            pred_acc_per_alpha[index_alpha] += (outputs_prob[i][pred].item()/\n",
    "                                            (outputs_prob[i][2].item() + outputs_prob[i][4].item()))\n",
    "                \n",
    "            #accuracy per noise\n",
    "            index_noise = int(2*noise[i]+0.1)\n",
    "            n_per_noise[index_noise] += 1\n",
    "            if label == pred:\n",
    "                acc_per_noise[index_noise] += 1\n",
    "            confidence_per_noise[index_noise] += (outputs_prob[i][pred].item()/\n",
    "                                            (outputs_prob[i][2].item() + outputs_prob[i][4].item()))\n",
    "        \n",
    "    accuracy = n_correct/n_samples\n",
    "    mean_loss = acc_loss/n_test_steps\n",
    "    #mean_pred_var = acc_pred_var/n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test trajectories: {accuracy*100}%')\n",
    "    print(f'Mean loss is: {mean_loss}')\n",
    "    #print(f'Mean Variance predicted by SWAG is: {mean_pred_var}')\n",
    "    \n",
    "    class_precision = n_class_truepositive/(n_class_truepositive+n_class_falsepositive)\n",
    "    class_recall = n_class_truepositive/(n_class_truepositive+n_class_falsenegative)\n",
    "    class_f1_score = n_class_truepositive/(n_class_truepositive+0.5*(n_class_falsepositive+n_class_falsenegative))\n",
    "    \n",
    "    for i in range(output_dim):\n",
    "        print(f\"F1 score of class {classes[i]} is {class_f1_score[i]}\")\n",
    "    print(f\"Mean F1 score is {class_f1_score.mean()}\")\n",
    "\n",
    "\n",
    "    \n",
    "confidence = confidence.to(\"cpu\")\n",
    "accuracy_interval = (accuracy_interval/n_interval).to(\"cpu\")\n",
    "n_interval = n_interval.to(\"cpu\")\n",
    "alpha_interval = alpha_interval.to(\"cpu\")\n",
    "acc_per_alpha = (acc_per_alpha/n_per_alpha).to(\"cpu\")\n",
    "pred_acc_per_alpha = (pred_acc_per_alpha/n_per_alpha).to(\"cpu\")\n",
    "n_per_alpha = n_per_alpha.to(\"cpu\")\n",
    "acc_per_noise = (acc_per_noise/n_per_noise).to(\"cpu\")\n",
    "confidence_per_noise = (confidence_per_noise/n_per_noise).to(\"cpu\")\n",
    "n_per_noise = n_per_noise.to(\"cpu\")\n",
    "\n",
    "    \n",
    "#plot confusion matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "conf_matrix = conf_matrix/conf_matrix.sum(axis=0)\n",
    "\n",
    "\n",
    "df_cm = pd.DataFrame(conf_matrix, index = [i for i in classes],\n",
    "                  columns = [i for i in classes])\n",
    "# plt.figure(figsize=(7,7))\n",
    "#sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a24c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 11})\n",
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "plt.rc('axes', titlesize=14)\n",
    "plt.rc('axes', labelsize=16)  \n",
    "\n",
    "plt.plot(alpha_interval+0.05,acc_per_alpha*100,\"o-\",linewidth=0.7,label = \"observed\")\n",
    "plt.plot(alpha_interval+0.05,pred_acc_per_alpha*100,\"o-\",linewidth=0.7, label = \"predicted\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Ground Truth Anomalous Exponent\")\n",
    "plt.ylabel(\"Accuracy %\")\n",
    "plt.legend()\n",
    "\n",
    "#use fitting save directory!\n",
    "#plt.savefig(\"figs/super_evaluate/class_acc_over_exp_sbmfbm_only_predicted.png\")\n",
    "#plt.savefig(\"figs/super_evaluate/class_acc_over_exp_sbmfbm_only_predicted.svg\")\n",
    "\n",
    "#plt.savefig(\"figs/super_evaluate/correctnoise/class_acc_over_exp_sbmfbm_only_predicted.png\")\n",
    "#plt.savefig(\"figs/super_evaluate/correctnoise/class_acc_over_exp_sbmfbm_only_predicted.svg\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea7b036",
   "metadata": {},
   "outputs": [],
   "source": [
    "snr = [10,2,1]\n",
    "x_labels = [\"SNR 10\", \"SNR 2\", \"SNR 1\"]\n",
    "x_val = [0,1,2]\n",
    "\n",
    "plt.plot(x_val,acc_per_noise*100,\"o\",ms = 11,label=\"Observed\")\n",
    "plt.plot(x_val,confidence_per_noise*100,\"o\",ms = 11,label=\"Predicted\")\n",
    "plt.xticks(ticks = x_val, labels = x_labels, size = 16)\n",
    "print(acc_per_noise)\n",
    "print(confidence_per_noise)\n",
    "plt.ylabel(\"Accuracy %\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "#use the correct save directory of the two below!\n",
    "#plt.savefig(\"figs/super_evaluate/class_acc_over_noise_sbmfbm_only.png\")\n",
    "#plt.savefig(\"figs/super_evaluate/class_acc_over_noise_sbmfbm_only.svg\")\n",
    "\n",
    "#plt.savefig(\"figs/super_evaluate/correctnoise/class_acc_over_noise_sbmfbm_only.png\")\n",
    "#plt.savefig(\"figs/super_evaluate/correctnoise/class_acc_over_noise_sbmfbm_only.svg\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a9efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(confidence+0.025,accuracy_interval,\"o-\")\n",
    "#plt.plot(confidence,accuracy_interval_multiswa,\"ro-\")\n",
    "plt.plot(np.arange(0,1.05,0.05),np.arange(0,1.05,0.05),\"grey\")\n",
    "plt.xlabel(\"Confidence\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "#plt.savefig(\"figs/super_evaluate/conf_acc_sbmfbm_only.png\")\n",
    "#plt.savefig(\"figs/super_evaluate/conf_acc_sbmfbm_only.svg\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed05f403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939f886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare to maximum achievable accuracy\n",
    "\n",
    "#savename = \"data/\"+\"datapoints\"+modelname[model]+f\"_T{T}_{N/1000}ktrajs_snr{snr}_diffcoeffnoise.txt\"\n",
    "\n",
    "#noise generated based generalized diff coeff\n",
    "\n",
    "#missclassification index\n",
    "T = 100\n",
    "alpha_range = np.append(np.arange(0.1,0.6,0.1),np.arange(0.6,0.8,0.05))\n",
    "alpha_range = np.append(alpha_range,np.arange(0.8,0.875,0.0125))\n",
    "alpha_range = np.append(alpha_range,np.arange(0.875,1.125,0.0125/4))\n",
    "alpha_range = np.append(alpha_range,np.arange(1.125,1.2,0.0125))\n",
    "alpha_range = np.append(alpha_range,np.arange(1.2,1.4,0.05))\n",
    "alpha_range = np.append(alpha_range,np.arange(1.4,1.91,0.1))\n",
    "alpha_range = alpha_range[alpha_range!=1]\n",
    "\n",
    "####SNR 1\n",
    "#if sbm=positive and fbm=negative\n",
    "#missclassifying sbm as fbm - false negative rate\n",
    "fn = np.loadtxt(\"../data/datapointsSBM_T100_10.0ktrajs_snr10_noiselenght100.txt\")/100\n",
    "fn_snr10 = fn\n",
    "#true positive rate, classifying sbm as sbm\n",
    "tp = 1-fn\n",
    "#missclassifying fbm as sbm - false positive rate\n",
    "fp = np.loadtxt(\"../data/datapointsFBM_T100_10.0ktrajs_snr10_noiselenght100.txt\")/100\n",
    "fp_snr10 = fp\n",
    "#true negative rate, classifying fbm as fbm\n",
    "tn = 1-fp\n",
    "\n",
    "#calculate the related scores\n",
    "F1_score_sbm = tp/(tp+0.5*(fp+fn)) #F1 score for sbm classification\n",
    "F1_score_fbm = tn/(tn+0.5*(fn+fp)) #F1 score for fbm classification\n",
    "F1_score = (F1_score_fbm+F1_score_sbm)/2 #mean F1 score\n",
    "acc = (tp+tn)/2\n",
    "acc_snr10 = acc\n",
    "\n",
    "####SNR 2\n",
    "#if sbm=positive and fbm=negative\n",
    "#missclassifying sbm as fbm - false negative rate\n",
    "fn = np.loadtxt(\"../data/datapointsSBM_T100_10.0ktrajs_snr2_noiselenght100.txt\")/100\n",
    "fn_snr2 = fn\n",
    "#true positive rate, classifying sbm as sbm\n",
    "tp = 1-fn\n",
    "#missclassifying fbm as sbm - false positive rate\n",
    "fp = np.loadtxt(\"../data/datapointsFBM_T100_10.0ktrajs_snr2_noiselenght100.txt\")/100\n",
    "fp_snr2 = fp\n",
    "#true negative rate, classifying fbm as fbm\n",
    "tn = 1-fp\n",
    "\n",
    "#calculate the related scores\n",
    "F1_score_sbm = tp/(tp+0.5*(fp+fn)) #F1 score for sbm classification\n",
    "F1_score_fbm = tn/(tn+0.5*(fn+fp)) #F1 score for fbm classification\n",
    "F1_score = (F1_score_fbm+F1_score_sbm)/2 #mean F1 score\n",
    "acc = (tp+tn)/2\n",
    "acc_snr2 = acc\n",
    "\n",
    "####SNR 10\n",
    "#if sbm=positive and fbm=negative\n",
    "#missclassifying sbm as fbm - false negative rate\n",
    "fn = np.loadtxt(\"../data/datapointsSBM_T100_10.0ktrajs_snr1_noiselenght100.txt\")/100\n",
    "fn_snr1 = fn\n",
    "#true positive rate, classifying sbm as sbm\n",
    "tp = 1-fn\n",
    "#missclassifying fbm as sbm - false positive rate\n",
    "fp = np.loadtxt(\"../data/datapointsFBM_T100_10.0ktrajs_snr1_noiselenght100.txt\")/100\n",
    "fp_snr1 = fp\n",
    "#true negative rate, classifying fbm as fbm\n",
    "tn = 1-fp\n",
    "\n",
    "#calculate the related scores\n",
    "F1_score_sbm = tp/(tp+0.5*(fp+fn)) #F1 score for sbm classification\n",
    "F1_score_fbm = tn/(tn+0.5*(fn+fp)) #F1 score for fbm classification\n",
    "F1_score = (F1_score_fbm+F1_score_sbm)/2 #mean F1 score\n",
    "acc = (tp+tn)/2\n",
    "acc_snr1 = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b9278",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 11})\n",
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "plt.rc('axes', titlesize=14)\n",
    "plt.rc('axes', labelsize=16)  \n",
    "\n",
    "plt.plot(alpha_interval+0.05,acc_per_alpha*100,\"o-\",linewidth=0.7,label = \"observed\")\n",
    "plt.plot(alpha_range,(acc_snr1+acc_snr2+acc_snr10)*100/3,\".-\",color=\"grey\",linewidth=0.7, label = \"maximum achievable\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Ground Truth Anomalous Exponent\")\n",
    "plt.ylabel(\"Accuracy %\")\n",
    "plt.legend()\n",
    "\n",
    "#use fitting save directory!\n",
    "#plt.savefig(\"figs/super_evaluate/class_acc_over_exp_sbmfbm_only_max_achieve.png\")\n",
    "#plt.savefig(\"figs/super_evaluate/class_acc_over_exp_sbmfbm_only_max_achieve.svg\")\n",
    "\n",
    "#plt.savefig(\"figs/super_evaluate/correctnoise/class_acc_over_exp_sbmfbm_max_achieve.png\")\n",
    "#plt.savefig(\"figs/super_evaluate/correctnoise/class_acc_over_exp_sbmfbm_max_achieve.svg\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a35e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
