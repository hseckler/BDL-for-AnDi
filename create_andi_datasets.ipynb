{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea658a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import andi_code as andi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a809e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "AD = andi.andi_datasets()\n",
    "AD.avail_models_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc3ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for our use only the first 3 boxes are needed, since we dont use complete datasets \n",
    "#but just the \"save_trajectories\" feature to quickly generate custom datasets when needed\n",
    "path = \"datasets/trajectories/2d/validset/\"\n",
    "n_samples = int(1e2)\n",
    "N_save = 2000#500#np.asarray([16000,16000,10000,16000,10000])#500#2000\n",
    "data = AD.andi_dataset(N = n_samples, tasks = [1, 2],\n",
    "                           dimensions = [2],\n",
    "                           load_dataset = False, save_dataset = False, save_trajectories = True, \n",
    "                           path_trajectories = path, N_save = N_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7d6cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dc77a6b",
   "metadata": {},
   "source": [
    "# create dataset with labels for alpha, model, uncertainty!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4595df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import andi as andi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143b9bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset with labels for alpha, model, uncertainty!\n",
    "#no weird shuffling is needed, this is done in training anyway\n",
    "def create_super_dataset(andi_dataset,T,N,dim,exponents,noise_T=None):\n",
    "    n_exp = len(exponents)\n",
    "    # Trajectories per model and exponent. Arbitrarely chosen to obtain balanced classes\n",
    "    n_per_model = np.ceil(1.6*N/5)\n",
    "    subdif, superdif = n_exp//2, n_exp//2+1\n",
    "    n_per_class =  np.zeros((andi_dataset.n_models, n_exp))\n",
    "    # ctrw, attm\n",
    "    n_per_class[:2, :subdif] = np.ceil(n_per_model/subdif)\n",
    "    # fbm\n",
    "    n_per_class[2, :] = np.ceil(n_per_model/(n_exp-1))\n",
    "    n_per_class[2, exponents == 2] = 0 # FBM can't be ballistic\n",
    "    # lw\n",
    "    n_per_class[3, subdif:] = np.ceil((n_per_model/superdif)*0.8)\n",
    "    # sbm\n",
    "    n_per_class[4, :] = np.ceil(n_per_model/n_exp)\n",
    "\n",
    "    if noise_T == None:\n",
    "        noise_T = T\n",
    "    #generate and normalize trajectory at noise_T, then cut down to T\n",
    "    #thereby generating a noise with respect to a trajectory of lenght noise_T\n",
    "    dataset = AD.create_dataset(T=noise_T, N=n_per_class, exponents=exponents, dimension=dim, \n",
    "                                models=np.arange(5))\n",
    "\n",
    "    # Normalize trajectories\n",
    "    n_traj = dataset.shape[0]\n",
    "    norm_trajs = andi.normalize(dataset[:, 2:].reshape(n_traj*dim, noise_T))\n",
    "    dataset[:, 2:] = norm_trajs.reshape(dataset[:, 2:].shape)\n",
    "    if noise_T != T:\n",
    "        #cut trajectories down to lenght T after normalization\n",
    "        dataset = dataset[:,0:T+2]\n",
    "    \n",
    "    \n",
    "    # Add localization error, Gaussian noise with sigma = [0.1, 0.5, 1]\n",
    "\n",
    "    loc_error_amplitude = np.random.choice(np.array([0.1, 0.5, 1]), size = n_traj*dim)\n",
    "    loc_error = (np.random.randn(n_traj*dim, int(T)).transpose()*loc_error_amplitude).transpose()\n",
    "\n",
    "    dataset = andi_dataset.create_noisy_localization_dataset(dataset, dimension = dim, T = T, noise_func = loc_error)\n",
    "    # Add random diffusion coefficients\n",
    "\n",
    "    trajs = dataset[:, 2:].reshape(n_traj*dim, T)\n",
    "    displacements = trajs[:, 1:] - trajs[:, :-1]\n",
    "    # Get new diffusion coefficients and displacements\n",
    "    diffusion_coefficients = np.random.randn(trajs.shape[0])\n",
    "    new_displacements = (displacements.transpose()*diffusion_coefficients).transpose()  \n",
    "    # Generate new trajectories and add to dataset\n",
    "    new_trajs = np.cumsum(new_displacements, axis = 1)\n",
    "    new_trajs = np.concatenate((np.zeros((new_trajs.shape[0], 1)), new_trajs), axis = 1)\n",
    "    dataset[:, 2:] = new_trajs.reshape(dataset[:, 2:].shape)\n",
    "    \n",
    "    #add noise value to dataset\n",
    "    dataset = np.concatenate((loc_error_amplitude.reshape(-1,1),dataset),axis=1)\n",
    "    #swap around to have fom [model,exponent,noise,x_0,x_1,...]\n",
    "    dataset[:,[0,2]] = dataset[:,[2,0]]\n",
    "    dataset[:,[0,1]] = dataset[:,[1,0]]\n",
    "    \n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d1ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AD = andi.andi_datasets()\n",
    "\n",
    "T = 500\n",
    "noise_T = T\n",
    "N = 500000\n",
    "dim = 1\n",
    "exponents = np.arange(0.05, 2.01, 0.05)\n",
    "\n",
    "#output dataset is [model,exponent,noise,x_1,x_2,...]\n",
    "dataset = create_super_dataset(AD,T,N,dim,exponents,noise_T=noise_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"datasets/super/1dim_{T}lenght/\"\n",
    "import os\n",
    "try:\n",
    "    os.mkdir(path)\n",
    "except:\n",
    "    print(\"directory exists\")\n",
    "np.savetxt(path + f\"andiset{N}.txt\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5466f779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9090a7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "len(np.arange(0.05, 2.01, 0.05))*10000*3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d83883c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b85aab29",
   "metadata": {},
   "source": [
    "#### FBM vs SBM only datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import andi as andi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce08f4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an sbm vs fbm only dataset\n",
    "#create dataset with labels for alpha, model, uncertainty!\n",
    "#no weird shuffling is needed, this is done in training anyway\n",
    "def create_super_dataset_sbmfbm_only(andi_dataset,T,N,dim,exponents,noise_T=None):\n",
    "    n_exp = len(exponents)\n",
    "    # Trajectories per model and exponent. Arbitrarely chosen to obtain balanced classes\n",
    "    n_per_class = int(np.ceil(N/(2*n_exp)))\n",
    "    \n",
    "    if noise_T == None:\n",
    "        noise_T = T\n",
    "    dataset = AD.create_dataset(T=noise_T, N=n_per_class, exponents=exponents, dimension=dim, \n",
    "                             models=[2,4])\n",
    "\n",
    "   \n",
    "\n",
    "    # Normalize trajectories\n",
    "    n_traj = dataset.shape[0]\n",
    "    norm_trajs = andi.normalize(dataset[:, 2:].reshape(n_traj*dim, noise_T))\n",
    "    dataset[:, 2:] = norm_trajs.reshape(dataset[:, 2:].shape)\n",
    "    if noise_T != T:\n",
    "        #cut trajectories down to lenght T after normalization\n",
    "        dataset = dataset[:,0:T+2]\n",
    "    # Add localization error, Gaussian noise with sigma = [0.1, 0.5, 1]\n",
    "\n",
    "    loc_error_amplitude = np.random.choice(np.array([0.1, 0.5, 1]), size = n_traj*dim)\n",
    "    loc_error = (np.random.randn(n_traj*dim, int(T)).transpose()*loc_error_amplitude).transpose()\n",
    "\n",
    "    dataset = andi_dataset.create_noisy_localization_dataset(dataset, dimension = dim, T = T, noise_func = loc_error)\n",
    "    # Add random diffusion coefficients\n",
    "\n",
    "    trajs = dataset[:, 2:].reshape(n_traj*dim, T)\n",
    "    displacements = trajs[:, 1:] - trajs[:, :-1]\n",
    "    # Get new diffusion coefficients and displacements\n",
    "    diffusion_coefficients = np.random.randn(trajs.shape[0])\n",
    "    new_displacements = (displacements.transpose()*diffusion_coefficients).transpose()  \n",
    "    # Generate new trajectories and add to dataset\n",
    "    new_trajs = np.cumsum(new_displacements, axis = 1)\n",
    "    new_trajs = np.concatenate((np.zeros((new_trajs.shape[0], 1)), new_trajs), axis = 1)\n",
    "    dataset[:, 2:] = new_trajs.reshape(dataset[:, 2:].shape)\n",
    "    \n",
    "    #add noise value to dataset\n",
    "    dataset = np.concatenate((loc_error_amplitude.reshape(-1,1),dataset),axis=1)\n",
    "    #swap around to have fom [model,exponent,noise,x_0,x_1,...]\n",
    "    dataset[:,[0,2]] = dataset[:,[2,0]]\n",
    "    dataset[:,[0,1]] = dataset[:,[1,0]]\n",
    "    \n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd3ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "AD = andi.andi_datasets()\n",
    "\n",
    "T = 100\n",
    "noise_T = T\n",
    "N = 50000\n",
    "dim = 1\n",
    "exponents = np.arange(0.05, 2, 0.05)\n",
    "\n",
    "#output dataset is [model,exponent,noise,x_1,x_2,...]\n",
    "dataset = create_super_dataset_sbmfbm_only(AD,T,N,dim,exponents,noise_T = noise_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1980c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/super/1dim_100lenght/\"\n",
    "np.savetxt(path + \"andiset50000_sbmfbm.txt\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e161adb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e74f841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e46688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b54a3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_andi_dataset import *\n",
    "\n",
    "T=100\n",
    "noise_T = T\n",
    "N_train = int(3e5)\n",
    "dim = 1\n",
    "use_increments = True\n",
    "model = 2\n",
    "N_save = [16000,16000,10000,16000,1000]\n",
    "\n",
    "#loading from saved trajectories, allows for only one dataset of trajectories usable for all trajectory lenghts\n",
    "train_path = \"datasets/trajectories/\"\n",
    "train_dataset = SingleModel_dataset_from_saved_trajs(path = train_path, task = 1, dim = dim, N_total = N_train,\n",
    "                                                  T = T, N_save = N_save[model], \n",
    "                                                  use_increments = use_increments, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f724c62f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9000a152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
